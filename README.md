# SnowSelected Paper List

Here is an academic paper list which contains the papers that [SnowCloud.ai](https://www.snowcloud.ai) AI Research Lab considered to be *very important, must read*.

The reason of any paper to be selected in this list may be any of the following:

1. The paper had brought a paradigm shift in its own domain.

2. The paper contained vital parts which lead the appearance of papers in 1.

3. The SnowCloud AI Lab considered that the paper may cause a paradigm shift within 3-5 years.

**SnowSelected  is all you need.**

## Natual Language Processing

* [Long and Short-Term Memory](https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735) : A well-known solution for long sentences processing.
* [Learning Longer Memory in RNN](https://arxiv.org/pdf/1412.7753.pdf)
* [RAM](https://arxiv.org/pdf/1406.6247.pdf)
* [Encoder-Decoder for NLP](https://arxiv.org/pdf/1406.1078.pdf)
- [Seq2Seq](https://arxiv.org/pdf/1409.3215.pdf)
- [A Convolutional Neural Network for Modelling Sentences](https://arxiv.org/pdf/1404.2188.pdf)
- [CNN on Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)
- [Very Deep Convolutional Networks
   for Text Classification](https://arxiv.org/pdf/1606.01781.pdf)
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf) : Attention first introduced in NLP field.
- [Soft And Hard Attention](https://arxiv.org/pdf/1502.03044.pdf)
- [Global And Local Attention](https://arxiv.org/pdf/1508.04025.pdf)
- [Character-Aware Neural Language Models](https://arxiv.org/pdf/1508.06615.pdf)
- [Attention is All You Need.](https://arxiv.org/pdf/1706.03762.pdf) : first transduction model relying
    entirely on self-attention to compute representations of its input and output without using RNNs or
    convolution.
- [BERT](https://arxiv.org/abs/1810.04805)
- [Transformer-XL](https://arxiv.org/abs/1901.02860): Introduced relative positional encoding. Combined AR and AE models to BERT.
- [XLNet](https://arxiv.org/pdf/1906.08237.pdf) : Introduced DAG while learning parameters in sentence segments. Resolve the problem may caused by excessive long sentence.

## Computer Vision

- [AlexNet](https://dl.acm.org/citation.cfm?id=3065386):The Beginning of Deep Learning
- [First Attention Solution](https://arxiv.org/abs/1109.3737)
- [GoogLeNet](https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf)
- [VGG](https://arxiv.org/pdf/1409.1556.pdf)
- [SPP Net](https://arxiv.org/pdf/1406.4729.pdf)
- [Batch Normalization](https://arxiv.org/pdf/1502.03167.pdf)
- [Highway Networks](https://papers.nips.cc/paper/5850-training-very-deep-networks.pdf)
- [ResNet](https://arxiv.org/pdf/1512.03385.pdf)
- [Deep Neural Networks for Object Detection](https://pdfs.semanticscholar.org/713f/73ce5c3013d9fb796c21b981dc6629af0bd5.pdf)
- [Faster RCNN](https://arxiv.org/pdf/1506.01497.pdf)
- [Fully Convolutional Networks](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf)
- [DenseNet](https://arxiv.org/pdf/1605.07110.pdf)
- [UNet](https://arxiv.org/pdf/1505.04597.pdf)
- [FlowNet](https://arxiv.org/pdf/1504.06852.pdf) and [FlowNet2.0](https://arxiv.org/pdf/1612.01925.pdf)
- [Stacked Hourglass](https://arxiv.org/pdf/1603.06937.pdf)
- [YOLO9000](https://arxiv.org/pdf/1612.08242.pdf)
- [Segmentation is All You Need](https://arxiv.org/pdf/1904.13300.pdf)

## Optimization

- [On Optimization Methods for Deep Learning](http://ai.stanford.edu/~ang/papers/icml11-OptimizationForDeepLearning.pdf)
- [Adam Optimization](https://arxiv.org/pdf/1412.6980.pdf)

## GAN

- [VAE](https://arxiv.org/abs/1312.6114)
- [GAN](https://arxiv.org/pdf/1406.2661.pdf)
- [conditional GAN](https://arxiv.org/pdf/1411.1784.pdf)
- [Generalized Denoising Auto-Encoders as Generative Models](http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models.pdf)
- [LAPGAN](https://arxiv.org/pdf/1506.05751.pdf)
- [GAN for Combinatorial Optimization](https://arxiv.org/pdf/1509.09235.pdf)
- [A note on the evaluation of generative models](https://arxiv.org/pdf/1511.01844.pdf)
- [DCGAN](https://arxiv.org/pdf/1511.06434.pdf)
- [SRGAN](https://arxiv.org/pdf/1609.04802.pdf)
- [Pix2Pix](https://arxiv.org/pdf/1611.07004.pdf)
- [CycleGAN](https://arxiv.org/pdf/1703.10593.pdf)

## Transfer Learning

- [JMMD](https://www.cv-foundation.org/openaccess/content_iccv_2013/papers/Long_Transfer_Feature_Learning_2013_ICCV_paper.pdf)
- [Adaptation regularization](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.708.6330&rep=rep1&type=pdf)
- [Feature Ensemble Plus Sample Selection: Domain Adaptation for Sentiment Classification](http://www.nlpr.ia.ac.cn/2013papers/gjkw/gk107.pdf)
- [Net2Net](https://arxiv.org/pdf/1511.05641.pdf)

## Deep Representations

- [Better Mixing via Deep Representations](https://arxiv.org/pdf/1207.4404.pdf)
- [Provable Bounds for Learning Some Deep
   Representations](https://arxiv.org/pdf/1310.6343.pdf)

## Audio Processing

- [WaveNet](https://arxiv.org/pdf/1609.03499.pdf)
- [Deep Voice](https://arxiv.org/pdf/1702.07825.pdf)
- [WaveNet for Denoising](https://arxiv.org/pdf/1706.07162.pdf)

## Tricks

- [Dropout](http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf)
- [No More Pesky Learning Rates](https://arxiv.org/pdf/1206.1106.pdf)
- [Bag of Tricks for CV](https://arxiv.org/pdf/1812.01187.pdf)

