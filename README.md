# SnowSelect

Paper that must read. SnowSelect is all you need.

## Natual Language Processing

1. [Long and Short-Term Memory](https://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735) : A well-known solution for long sentences processing.
2. [Learning Longer Memory in RNN](https://arxiv.org/pdf/1412.7753.pdf)
3. [Seq2Seq](https://arxiv.org/pdf/1409.3215.pdf)
4. [CNN on Sentence Classification](https://arxiv.org/pdf/1408.5882.pdf)
5. [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/pdf/1409.0473.pdf) : Attention first introduced in NLP field.
6. [Attention is All You Need.](https://arxiv.org/abs/1706.03762) : first transduction model relying
   entirely on self-attention to compute representations of its input and output without using RNNs or
   convolution.
7. [BERT](https://arxiv.org/abs/1810.04805)
8. [Transformer-XL](https://arxiv.org/abs/1901.02860): 

## Computer Vision

1. [AlexNet](https://dl.acm.org/citation.cfm?id=3065386):The Beginning of Deep Learning
2. [First Attention Solution](https://arxiv.org/abs/1109.3737)
3. [GoogLeNet](https://www.cs.unc.edu/~wliu/papers/GoogLeNet.pdf)
4. [Highway Networks](https://papers.nips.cc/paper/5850-training-very-deep-networks.pdf)
5. [ResNet](https://arxiv.org/pdf/1512.03385.pdf)
6. [Faster RCNN](https://arxiv.org/pdf/1506.01497.pdf)
7. [UNet]
8. [Stacked Hourglass](https://arxiv.org/pdf/1603.06937.pdf)
9. [YOLO9000](https://arxiv.org/pdf/1612.08242)

## Optimization

1. [On Optimization Methods for Deep Learning](http://ai.stanford.edu/~ang/papers/icml11-OptimizationForDeepLearning.pdf)
2. [Adam Optimization](https://arxiv.org/pdf/1412.6980.pdf)
3. [VGG](https://arxiv.org/pdf/1409.1556.pdf)
4. [SPP Net](https://arxiv.org/pdf/1406.4729.pdf)

## GAN

1. [VAE](https://arxiv.org/abs/1312.6114)
2. [GAN](https://arxiv.org/pdf/1406.2661.pdf)
3. [conditional GAN](https://arxiv.org/pdf/1411.1784.pdf)
4. [Generalized Denoising Auto-Encoders as Generative Models](http://papers.nips.cc/paper/5023-generalized-denoising-auto-encoders-as-generative-models.pdf)

## Deep Representations

1. [Better Mixing via Deep Representations](https://arxiv.org/pdf/1207.4404.pdf)

## Tricks

1. [Dropout](http://papers.nips.cc/paper/4882-dropout-training-as-adaptive-regularization.pdf)
2. [No More Pesky Learning Rates](https://arxiv.org/pdf/1206.1106.pdf)
3. [Bag of Tricks for CV](https://arxiv.org/pdf/1812.01187.pdf)

## Audio Processing

1. [WaveNet](https://arxiv.org/pdf/1609.03499.pdf)
2. [Deep Voice](https://arxiv.org/pdf/1702.07825.pdf)
3. [WaveNet for Denoising](https://arxiv.org/pdf/1706.07162.pdf)